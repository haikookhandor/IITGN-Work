# Gradient Boosting
    n_estimators = 3
    lr = 0.9
    test_size = 0.2
    Root Mean Squared Error: 34.48
    MAE: 29.28

## Why Gradient Boosting

    Here the weak learner trains on the incorrectly classified data of a strong learner. In AdaBoost, we were training this on the entire distribution. Thus each time we compute the residual or wrongly classified samples. 